{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\t5_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TweetEval dataset\n",
    "tweet_eval_dataset = load_dataset('tweet_eval',\"emoji\")\n",
    "\n",
    "# Accessing different splits\n",
    "train_dataset = tweet_eval_dataset['train']\n",
    "test_dataset = tweet_eval_dataset['test']\n",
    "validation_dataset = tweet_eval_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Concatenate datasets together\n",
    "all_data = pd.concat([train_dataset.to_pandas(), validation_dataset.to_pandas(), test_dataset.to_pandas()])\n",
    "\n",
    "# Drop all labels\n",
    "all_data = all_data.drop(columns=['label'])\n",
    "all_data['text'] = all_data['text']\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove all characters except punctuation and English characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
    "    # 2. Remove all space at the beginning of the sentence\n",
    "    text = text.lstrip()\n",
    "    # 3. Remove all extra space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "all_data = all_data.head(100)\n",
    "texts = all_data['text'].tolist()\n",
    "\n",
    "\n",
    "texts = [clean_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5WithVAE were not initialized from the model checkpoint at t5-small and are newly initialized: ['to_style.weight', 'to_style.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class T5WithVAE(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.sequence_length = 128\n",
    "        self.feature_size = config.d_model\n",
    "        # here we assume that the input max length is 512, AND the latent size is 512\n",
    "        self.latent_size = self.sequence_length *  self.feature_size\n",
    "        # self.dense = torch.nn.Linear(self.latent_size, self.latent_size)\n",
    "        self.to_style = torch.nn.Linear(self.latent_size // 4, self.latent_size // 2)\n",
    "        \n",
    "        self.lm_head = torch.nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.ignore_index = -100  # usually the index for padding tokens in Hugging Face models\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.tie_weights()\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mean)\n",
    "    \n",
    "    def compute_loss(self, lm_logits, labels):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.ignore_index)\n",
    "        logits_flat = lm_logits.view(-1, self.vocab_size)\n",
    "        labels_flat = labels.view(-1)\n",
    "        return loss_fct(logits_flat, labels_flat)\n",
    "    \n",
    "    def forward(self,input_ids=None, **kwargs):\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids,return_dict=True)\n",
    "        latent_vector = encoder_outputs.last_hidden_state\n",
    "        # flatten the latent vector\n",
    "        latent_vector = latent_vector.view(latent_vector.size(0), -1)\n",
    "        #latent_vector = self.dense(latent_vector) \n",
    "        \n",
    "        # split the latent vector into three parts\n",
    "        # content_vector have first 1/2 size of the latent vector\n",
    "        # style_mean have second 1/4 size of the latent vector\n",
    "        # style_var have last 1/4 size of the latent vector\n",
    "        content_vector = latent_vector[:, :self.latent_size // 2]\n",
    "        style_mean = latent_vector[:, self.latent_size // 2: self.latent_size // 2 + self.latent_size // 4]\n",
    "        style_logvar  = latent_vector[:, self.latent_size // 2 + self.latent_size // 4:]\n",
    "        \n",
    "        # reparameterization trick\n",
    "        style_vector = self.reparameterize(style_mean, style_logvar )\n",
    "        style_vector = self.to_style(style_vector)\n",
    "        \n",
    "        # concatenate the content vector and style vector\n",
    "        combined_vector = torch.cat([content_vector, style_vector], dim=1)\n",
    "        combined_vector = combined_vector.view(combined_vector.size(0), self.sequence_length, self.feature_size)\n",
    "        \n",
    "        # decoder, the decoder_input_ids should be the same as the encoder input_ids\n",
    "        decoder_outputs = self.decoder(input_ids =input_ids, encoder_hidden_states=combined_vector, return_dict=True)\n",
    "        sequence_output = decoder_outputs[0]\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "        \n",
    "        # calculate the loss\n",
    "        #lm_logits = decoder_outputs.logits\n",
    "        reconstruction_loss = self.compute_loss(\n",
    "            lm_logits, \n",
    "            input_ids)\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        kl_loss = -0.5 * torch.sum(1 + style_logvar - style_mean.pow(2) - style_logvar.exp())\n",
    "    \n",
    "        # Combine the losses\n",
    "        loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        return loss, decoder_outputs\n",
    "    \n",
    "    def generate(self, input_ids=None, **kwargs):\n",
    "        decoder_outputs = self.decoder(input_ids=input_ids, return_dict=True)\n",
    "        sequence_output = decoder_outputs[0]\n",
    "        lm_logits = self.lm_head(sequence_output)\n",
    "        return lm_logits\n",
    "    \n",
    "    \n",
    "model = T5WithVAE.from_pretrained('t5-small').to(device)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 4\n",
    "dataset = TensorDataset(input_ids)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Train Loss: 2400.233\n",
      "Epoch 2/3 | Train Loss: 2084.003\n",
      "Epoch 3/3 | Train Loss: 1773.296\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.train()\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Tracking variables\n",
    "    train_loss = 0\n",
    "    for batch in loader:\n",
    "        # Assuming that 'batch' is a tuple of (input_ids, labels)\n",
    "        input_ids = batch[0].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss, outputs = model(input_ids=input_ids)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update training loss\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss over the training data\n",
    "    avg_train_loss = train_loss / len(loader)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

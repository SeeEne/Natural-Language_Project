{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TweetEval dataset\n",
    "tweet_eval_dataset = load_dataset('tweet_eval',\"emoji\")\n",
    "\n",
    "# Accessing different splits\n",
    "train_dataset = tweet_eval_dataset['train']\n",
    "test_dataset = tweet_eval_dataset['test']\n",
    "validation_dataset = tweet_eval_dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Concatenate datasets together\n",
    "all_data = pd.concat([train_dataset.to_pandas(), validation_dataset.to_pandas(), test_dataset.to_pandas()])\n",
    "\n",
    "# Drop all labels\n",
    "all_data = all_data.drop(columns=['label'])\n",
    "all_data['text'] = all_data['text']\n",
    "\n",
    "def clean_text(text):\n",
    "    # 1. Remove all characters except punctuation and English characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
    "    # 2. Remove all space at the beginning of the sentence\n",
    "    text = text.lstrip()\n",
    "    # 3. Remove all extra space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "all_data = all_data.head(2000)\n",
    "texts = all_data['text'].tolist()\n",
    "\n",
    "\n",
    "texts = [clean_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 128\n",
    "dataset = TensorDataset(input_ids)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.4507443755865097\n",
      "Epoch: 2, Loss: 2.172430247068405\n",
      "Epoch: 3, Loss: 1.9878387451171875\n",
      "Epoch: 4, Loss: 1.8234905824065208\n",
      "Epoch: 5, Loss: 1.6832695826888084\n",
      "Epoch: 6, Loss: 1.5397050827741623\n",
      "Epoch: 7, Loss: 1.3902331590652466\n",
      "Epoch: 8, Loss: 1.2563733607530594\n",
      "Epoch: 9, Loss: 1.176120601594448\n",
      "Epoch: 10, Loss: 1.0549101531505585\n",
      "Epoch: 11, Loss: 0.9549500122666359\n",
      "Epoch: 12, Loss: 0.8542642258107662\n",
      "Epoch: 13, Loss: 0.7640678025782108\n",
      "Epoch: 14, Loss: 0.6828804537653923\n",
      "Epoch: 15, Loss: 0.6154415309429169\n",
      "Epoch: 16, Loss: 0.5691859051585197\n",
      "Epoch: 17, Loss: 0.5383298471570015\n",
      "Epoch: 18, Loss: 0.506182448938489\n",
      "Epoch: 19, Loss: 0.4697606787085533\n",
      "Epoch: 20, Loss: 0.4358366262167692\n",
      "Epoch: 21, Loss: 0.40659354254603386\n",
      "Epoch: 22, Loss: 0.3912338316440582\n",
      "Epoch: 23, Loss: 0.36151413433253765\n",
      "Epoch: 24, Loss: 0.3417316656559706\n",
      "Epoch: 25, Loss: 0.30991453118622303\n",
      "Epoch: 26, Loss: 0.2843216471374035\n",
      "Epoch: 27, Loss: 0.2661282438784838\n",
      "Epoch: 28, Loss: 0.2521718069911003\n",
      "Epoch: 29, Loss: 0.23461493011564016\n",
      "Epoch: 30, Loss: 0.22525148279964924\n",
      "Epoch: 31, Loss: 0.20516910590231419\n",
      "Epoch: 32, Loss: 0.20502824895083904\n",
      "Epoch: 33, Loss: 0.18528250884264708\n",
      "Epoch: 34, Loss: 0.18153333943337202\n",
      "Epoch: 35, Loss: 0.15829959884285927\n",
      "Epoch: 36, Loss: 0.16478736232966185\n",
      "Epoch: 37, Loss: 0.14982634410262108\n",
      "Epoch: 38, Loss: 0.1479750769212842\n",
      "Epoch: 39, Loss: 0.13781660562381148\n",
      "Epoch: 40, Loss: 0.13053063349798322\n",
      "Epoch: 41, Loss: 0.12490758765488863\n",
      "Epoch: 42, Loss: 0.12741852970793843\n",
      "Epoch: 43, Loss: 0.1171732279472053\n",
      "Epoch: 44, Loss: 0.12125959573313594\n",
      "Epoch: 45, Loss: 0.11257804976776242\n",
      "Epoch: 46, Loss: 0.10979897249490023\n",
      "Epoch: 47, Loss: 0.10593583900481462\n",
      "Epoch: 48, Loss: 0.10592397162690759\n",
      "Epoch: 49, Loss: 0.10775666404515505\n",
      "Epoch: 50, Loss: 0.09670175518840551\n",
      "Epoch: 51, Loss: 0.09453970054164529\n",
      "Epoch: 52, Loss: 0.09191876510158181\n",
      "Epoch: 53, Loss: 0.08989466819912195\n",
      "Epoch: 54, Loss: 0.09074490144848824\n",
      "Epoch: 55, Loss: 0.08716890774667263\n",
      "Epoch: 56, Loss: 0.08078247727826238\n",
      "Epoch: 57, Loss: 0.08292058389633894\n",
      "Epoch: 58, Loss: 0.07864915998652577\n",
      "Epoch: 59, Loss: 0.07492894981987774\n",
      "Epoch: 60, Loss: 0.07606532424688339\n",
      "Epoch: 61, Loss: 0.07107559801079333\n",
      "Epoch: 62, Loss: 0.0693767131306231\n",
      "Epoch: 63, Loss: 0.06629676232114434\n",
      "Epoch: 64, Loss: 0.06640101852826774\n",
      "Epoch: 65, Loss: 0.06741492287255824\n",
      "Epoch: 66, Loss: 0.06428417167626321\n",
      "Epoch: 67, Loss: 0.06250499677844346\n",
      "Epoch: 68, Loss: 0.05929201957769692\n",
      "Epoch: 69, Loss: 0.05824554688297212\n",
      "Epoch: 70, Loss: 0.05313139199279249\n",
      "Epoch: 71, Loss: 0.05730903777293861\n",
      "Epoch: 72, Loss: 0.053319191792979836\n",
      "Epoch: 73, Loss: 0.05351158347912133\n",
      "Epoch: 74, Loss: 0.05549169238656759\n",
      "Epoch: 75, Loss: 0.05067556374706328\n",
      "Epoch: 76, Loss: 0.054943378549069166\n",
      "Epoch: 77, Loss: 0.0476819674950093\n",
      "Epoch: 78, Loss: 0.04923561681061983\n",
      "Epoch: 79, Loss: 0.05137526779435575\n",
      "Epoch: 80, Loss: 0.048105038702487946\n",
      "Epoch: 81, Loss: 0.045410921797156334\n",
      "Epoch: 82, Loss: 0.04555286676622927\n",
      "Epoch: 83, Loss: 0.04321464244276285\n",
      "Epoch: 84, Loss: 0.0455201982986182\n",
      "Epoch: 85, Loss: 0.044798896880820394\n",
      "Epoch: 86, Loss: 0.0398993274429813\n",
      "Epoch: 87, Loss: 0.042537301080301404\n",
      "Epoch: 88, Loss: 0.043897645198740065\n",
      "Epoch: 89, Loss: 0.038969447603449225\n",
      "Epoch: 90, Loss: 0.04113433673046529\n",
      "Epoch: 91, Loss: 0.039275022922083735\n",
      "Epoch: 92, Loss: 0.04044436337426305\n",
      "Epoch: 93, Loss: 0.03836525569204241\n",
      "Epoch: 94, Loss: 0.03808355866931379\n",
      "Epoch: 95, Loss: 0.036994439316913486\n",
      "Epoch: 96, Loss: 0.036757401074282825\n",
      "Epoch: 97, Loss: 0.03629766230005771\n",
      "Epoch: 98, Loss: 0.03460865409579128\n",
      "Epoch: 99, Loss: 0.036330218077637255\n",
      "Epoch: 100, Loss: 0.033650903729721904\n",
      "Epoch: 101, Loss: 0.034458187175914645\n",
      "Epoch: 102, Loss: 0.033535069203935564\n",
      "Epoch: 103, Loss: 0.033072445658035576\n",
      "Epoch: 104, Loss: 0.0339692416600883\n",
      "Epoch: 105, Loss: 0.03167713026050478\n",
      "Epoch: 106, Loss: 0.03300292696803808\n",
      "Epoch: 107, Loss: 0.030852890689857304\n",
      "Epoch: 108, Loss: 0.03115946939215064\n",
      "Epoch: 109, Loss: 0.031841973890550435\n",
      "Epoch: 110, Loss: 0.0305955110816285\n",
      "Epoch: 111, Loss: 0.030759767163544893\n",
      "Epoch: 112, Loss: 0.029910777928307652\n",
      "Epoch: 113, Loss: 0.031552184955216944\n",
      "Epoch: 114, Loss: 0.0294461059384048\n",
      "Epoch: 115, Loss: 0.026968798134475946\n",
      "Epoch: 116, Loss: 0.02972118800971657\n",
      "Epoch: 117, Loss: 0.028749695396982133\n",
      "Epoch: 118, Loss: 0.025607578398194164\n",
      "Epoch: 119, Loss: 0.026049230014905334\n",
      "Epoch: 120, Loss: 0.028577656717970967\n",
      "Epoch: 121, Loss: 0.027335449238307774\n",
      "Epoch: 122, Loss: 0.02989239862654358\n",
      "Epoch: 123, Loss: 0.025041886954568326\n",
      "Epoch: 124, Loss: 0.026218502200208604\n",
      "Epoch: 125, Loss: 0.024476825958117843\n",
      "Epoch: 126, Loss: 0.027432011906057596\n",
      "Epoch: 127, Loss: 0.025199324474669993\n",
      "Epoch: 128, Loss: 0.024524398730136454\n",
      "Epoch: 129, Loss: 0.023374063661321998\n",
      "Epoch: 130, Loss: 0.02357149322051555\n",
      "Epoch: 131, Loss: 0.024353139626327902\n",
      "Epoch: 132, Loss: 0.02221201069187373\n",
      "Epoch: 133, Loss: 0.023826575023122132\n",
      "Epoch: 134, Loss: 0.024270514608360827\n",
      "Epoch: 135, Loss: 0.023709377041086555\n",
      "Epoch: 136, Loss: 0.023181327851489186\n",
      "Epoch: 137, Loss: 0.021059361868537962\n",
      "Epoch: 138, Loss: 0.021994455950334668\n",
      "Epoch: 139, Loss: 0.0208501074812375\n",
      "Epoch: 140, Loss: 0.02322584472130984\n",
      "Epoch: 141, Loss: 0.0226255344459787\n",
      "Epoch: 142, Loss: 0.01963996683480218\n",
      "Epoch: 143, Loss: 0.020040590490680188\n",
      "Epoch: 144, Loss: 0.020817915443331003\n",
      "Epoch: 145, Loss: 0.02187652583234012\n",
      "Epoch: 146, Loss: 0.018526646483223885\n",
      "Epoch: 147, Loss: 0.01860098063480109\n",
      "Epoch: 148, Loss: 0.02089879516279325\n",
      "Epoch: 149, Loss: 0.019564200018066913\n",
      "Epoch: 150, Loss: 0.02190150402020663\n",
      "Epoch: 151, Loss: 0.018600659328512847\n",
      "Epoch: 152, Loss: 0.018717087747063488\n",
      "Epoch: 153, Loss: 0.019417275092564523\n",
      "Epoch: 154, Loss: 0.01835684129036963\n",
      "Epoch: 155, Loss: 0.019697191775776446\n",
      "Epoch: 156, Loss: 0.017268384515773505\n",
      "Epoch: 157, Loss: 0.01888304331805557\n",
      "Epoch: 158, Loss: 0.020031759806443006\n",
      "Epoch: 159, Loss: 0.018338287598453462\n",
      "Epoch: 160, Loss: 0.01792822330025956\n",
      "Epoch: 161, Loss: 0.017798054730519652\n",
      "Epoch: 162, Loss: 0.018838470452465117\n",
      "Epoch: 163, Loss: 0.017140605370514095\n",
      "Epoch: 164, Loss: 0.01707050547702238\n",
      "Epoch: 165, Loss: 0.017579295148607343\n",
      "Epoch: 166, Loss: 0.017438003967981786\n",
      "Epoch: 167, Loss: 0.016879617294762284\n",
      "Epoch: 168, Loss: 0.0167399876518175\n",
      "Epoch: 169, Loss: 0.016081094916444272\n",
      "Epoch: 170, Loss: 0.016494475188665092\n",
      "Epoch: 171, Loss: 0.01694949867669493\n",
      "Epoch: 172, Loss: 0.014697572798468173\n",
      "Epoch: 173, Loss: 0.017217989021446556\n",
      "Epoch: 174, Loss: 0.016328112804330885\n",
      "Epoch: 175, Loss: 0.01531715941382572\n",
      "Epoch: 176, Loss: 0.014929794298950583\n",
      "Epoch: 177, Loss: 0.01608814758947119\n",
      "Epoch: 178, Loss: 0.015764349256642163\n",
      "Epoch: 179, Loss: 0.015129431150853634\n",
      "Epoch: 180, Loss: 0.015454371343366802\n",
      "Epoch: 181, Loss: 0.015234043006785214\n",
      "Epoch: 182, Loss: 0.014686629176139832\n",
      "Epoch: 183, Loss: 0.015140204108320177\n",
      "Epoch: 184, Loss: 0.016141563071869314\n",
      "Epoch: 185, Loss: 0.013791955192573369\n",
      "Epoch: 186, Loss: 0.014213570713764057\n",
      "Epoch: 187, Loss: 0.014324361167382449\n",
      "Epoch: 188, Loss: 0.01631132105831057\n",
      "Epoch: 189, Loss: 0.013977526919916272\n",
      "Epoch: 190, Loss: 0.015833229757845402\n",
      "Epoch: 191, Loss: 0.013811034907121211\n",
      "Epoch: 192, Loss: 0.01651928835781291\n",
      "Epoch: 193, Loss: 0.012891252059489489\n",
      "Epoch: 194, Loss: 0.015291175397578627\n",
      "Epoch: 195, Loss: 0.012947670416906476\n",
      "Epoch: 196, Loss: 0.013163703319150954\n",
      "Epoch: 197, Loss: 0.013951618690043688\n",
      "Epoch: 198, Loss: 0.013274854194605723\n",
      "Epoch: 199, Loss: 0.013369248539675027\n",
      "Epoch: 200, Loss: 0.013727074139751494\n",
      "Epoch: 201, Loss: 0.013648778898641467\n",
      "Epoch: 202, Loss: 0.012535773275885731\n",
      "Epoch: 203, Loss: 0.012437099707312882\n",
      "Epoch: 204, Loss: 0.013547992508392781\n",
      "Epoch: 205, Loss: 0.013632137794047594\n",
      "Epoch: 206, Loss: 0.013691018393728882\n",
      "Epoch: 207, Loss: 0.011888824548805133\n",
      "Epoch: 208, Loss: 0.013268326641991735\n",
      "Epoch: 209, Loss: 0.011441250215284526\n",
      "Epoch: 210, Loss: 0.012699553684797138\n",
      "Epoch: 211, Loss: 0.010941057465970516\n",
      "Epoch: 212, Loss: 0.014279240393079817\n",
      "Epoch: 213, Loss: 0.012830288877012208\n",
      "Epoch: 214, Loss: 0.012568546313559636\n",
      "Epoch: 215, Loss: 0.011717495217453688\n",
      "Epoch: 216, Loss: 0.012586071330588311\n",
      "Epoch: 217, Loss: 0.011412001913413405\n",
      "Epoch: 218, Loss: 0.012256402376806363\n",
      "Epoch: 219, Loss: 0.012953779310919344\n",
      "Epoch: 220, Loss: 0.012161365651991218\n",
      "Epoch: 221, Loss: 0.011713737389072776\n",
      "Epoch: 222, Loss: 0.011842049221741036\n",
      "Epoch: 223, Loss: 0.011829923838376999\n",
      "Epoch: 224, Loss: 0.011548080772627145\n",
      "Epoch: 225, Loss: 0.011896444048034027\n",
      "Epoch: 226, Loss: 0.011774546117521822\n",
      "Epoch: 227, Loss: 0.012219432683195919\n",
      "Epoch: 228, Loss: 0.011795203579822555\n",
      "Epoch: 229, Loss: 0.010836761677637696\n",
      "Epoch: 230, Loss: 0.01207292010076344\n",
      "Epoch: 231, Loss: 0.011188324424438179\n",
      "Epoch: 232, Loss: 0.009713675128296018\n",
      "Epoch: 233, Loss: 0.011163457675138488\n",
      "Epoch: 234, Loss: 0.012477778363972902\n",
      "Epoch: 235, Loss: 0.010116084507899359\n",
      "Epoch: 236, Loss: 0.009336706309113652\n",
      "Epoch: 237, Loss: 0.010642654029652476\n",
      "Epoch: 238, Loss: 0.011766064213588834\n",
      "Epoch: 239, Loss: 0.009685734141385183\n",
      "Epoch: 240, Loss: 0.010218245384749025\n",
      "Epoch: 241, Loss: 0.010842465911991894\n",
      "Epoch: 242, Loss: 0.009154562168987468\n",
      "Epoch: 243, Loss: 0.010670989024220034\n",
      "Epoch: 244, Loss: 0.01017552160192281\n",
      "Epoch: 245, Loss: 0.010323908732971177\n",
      "Epoch: 246, Loss: 0.009589632449205965\n",
      "Epoch: 247, Loss: 0.009872980270301923\n",
      "Epoch: 248, Loss: 0.010035066516138613\n",
      "Epoch: 249, Loss: 0.009941705618984997\n",
      "Epoch: 250, Loss: 0.009094295441173017\n",
      "Epoch: 251, Loss: 0.009033104288391769\n",
      "Epoch: 252, Loss: 0.009168710646918043\n",
      "Epoch: 253, Loss: 0.009501465799985453\n",
      "Epoch: 254, Loss: 0.009443725139135495\n",
      "Epoch: 255, Loss: 0.009042408782988787\n",
      "Epoch: 256, Loss: 0.0098648963321466\n",
      "Epoch: 257, Loss: 0.009592551330570132\n",
      "Epoch: 258, Loss: 0.01004069487680681\n",
      "Epoch: 259, Loss: 0.009705713600851595\n",
      "Epoch: 260, Loss: 0.009106476500164717\n",
      "Epoch: 261, Loss: 0.00940753670874983\n",
      "Epoch: 262, Loss: 0.009338189207483083\n",
      "Epoch: 263, Loss: 0.009203550085658208\n",
      "Epoch: 264, Loss: 0.008560551883419976\n",
      "Epoch: 265, Loss: 0.00904475903371349\n",
      "Epoch: 266, Loss: 0.008363968838239089\n",
      "Epoch: 267, Loss: 0.008483083744067699\n",
      "Epoch: 268, Loss: 0.007913953391835093\n",
      "Epoch: 269, Loss: 0.007964059361256659\n",
      "Epoch: 270, Loss: 0.007823100953828543\n",
      "Epoch: 271, Loss: 0.009681172639830038\n",
      "Epoch: 272, Loss: 0.008823657903121784\n",
      "Epoch: 273, Loss: 0.007995889463927597\n",
      "Epoch: 274, Loss: 0.008353185839951038\n",
      "Epoch: 275, Loss: 0.008791583823040128\n",
      "Epoch: 276, Loss: 0.008344398229382932\n",
      "Epoch: 277, Loss: 0.009016708674607798\n",
      "Epoch: 278, Loss: 0.0087792856793385\n",
      "Epoch: 279, Loss: 0.007546633743913844\n",
      "Epoch: 280, Loss: 0.008456653624307364\n",
      "Epoch: 281, Loss: 0.008100737351924181\n",
      "Epoch: 282, Loss: 0.007933309738291427\n",
      "Epoch: 283, Loss: 0.007853330782381818\n",
      "Epoch: 284, Loss: 0.00837275804951787\n",
      "Epoch: 285, Loss: 0.007729333563474938\n",
      "Epoch: 286, Loss: 0.009037124691531062\n",
      "Epoch: 287, Loss: 0.007493317883927375\n",
      "Epoch: 288, Loss: 0.008518171380273998\n",
      "Epoch: 289, Loss: 0.007481376233045012\n",
      "Epoch: 290, Loss: 0.007805974601069465\n",
      "Epoch: 291, Loss: 0.006600626322324388\n",
      "Epoch: 292, Loss: 0.007004397251876071\n",
      "Epoch: 293, Loss: 0.007108144913217984\n",
      "Epoch: 294, Loss: 0.007941619711345993\n",
      "Epoch: 295, Loss: 0.007149463897803798\n",
      "Epoch: 296, Loss: 0.007415725820465013\n",
      "Epoch: 297, Loss: 0.008593279475462623\n",
      "Epoch: 298, Loss: 0.007247636967804283\n",
      "Epoch: 299, Loss: 0.006871419289382175\n",
      "Epoch: 300, Loss: 0.006823476665886119\n",
      "Epoch: 301, Loss: 0.007425449759466574\n",
      "Epoch: 302, Loss: 0.006118406599853188\n",
      "Epoch: 303, Loss: 0.009028966625919566\n",
      "Epoch: 304, Loss: 0.007442584057571366\n",
      "Epoch: 305, Loss: 0.007201977306976914\n",
      "Epoch: 306, Loss: 0.006379258105880581\n",
      "Epoch: 307, Loss: 0.007091397215845063\n",
      "Epoch: 308, Loss: 0.007305294260731898\n",
      "Epoch: 309, Loss: 0.007297536707483232\n",
      "Epoch: 310, Loss: 0.006614440164412372\n",
      "Epoch: 311, Loss: 0.006489687919383869\n",
      "Epoch: 312, Loss: 0.0070114946574904025\n",
      "Epoch: 313, Loss: 0.006933945958735421\n",
      "Epoch: 314, Loss: 0.007100273258402012\n",
      "Epoch: 315, Loss: 0.006710697300150059\n",
      "Epoch: 316, Loss: 0.006779700634069741\n",
      "Epoch: 317, Loss: 0.00642155269451905\n",
      "Epoch: 318, Loss: 0.006997491174843162\n",
      "Epoch: 319, Loss: 0.005946608318481594\n",
      "Epoch: 320, Loss: 0.006517964997328818\n",
      "Epoch: 321, Loss: 0.007030236884020269\n",
      "Epoch: 322, Loss: 0.006095408854889683\n",
      "Epoch: 323, Loss: 0.006470480366260745\n",
      "Epoch: 324, Loss: 0.005706370386178605\n",
      "Epoch: 325, Loss: 0.006232609550352208\n",
      "Epoch: 326, Loss: 0.006550961566972546\n",
      "Epoch: 327, Loss: 0.006734057504218072\n",
      "Epoch: 328, Loss: 0.006167422950966284\n",
      "Epoch: 329, Loss: 0.0060995277017354965\n",
      "Epoch: 330, Loss: 0.00594625566736795\n",
      "Epoch: 331, Loss: 0.006852725404314697\n",
      "Epoch: 332, Loss: 0.006314491489320062\n",
      "Epoch: 333, Loss: 0.007324138583499007\n",
      "Epoch: 334, Loss: 0.0063941613043425605\n",
      "Epoch: 335, Loss: 0.004894278492429294\n",
      "Epoch: 336, Loss: 0.006184087294968776\n",
      "Epoch: 337, Loss: 0.005918142705922946\n",
      "Epoch: 338, Loss: 0.005459936539409682\n",
      "Epoch: 339, Loss: 0.006655588702415116\n",
      "Epoch: 340, Loss: 0.005819323458126746\n",
      "Epoch: 341, Loss: 0.005901950775296427\n",
      "Epoch: 342, Loss: 0.0053830571414437145\n",
      "Epoch: 343, Loss: 0.005314080262905918\n",
      "Epoch: 344, Loss: 0.006248341087484732\n",
      "Epoch: 345, Loss: 0.005561422032769769\n",
      "Epoch: 346, Loss: 0.006193070934386924\n",
      "Epoch: 347, Loss: 0.006010226687067188\n",
      "Epoch: 348, Loss: 0.00623253126104828\n",
      "Epoch: 349, Loss: 0.0053990893211448565\n",
      "Epoch: 350, Loss: 0.00506870100798551\n",
      "Epoch: 351, Loss: 0.006126703243353404\n",
      "Epoch: 352, Loss: 0.0057823948009172454\n",
      "Epoch: 353, Loss: 0.0057604095491115\n",
      "Epoch: 354, Loss: 0.005399912915891036\n",
      "Epoch: 355, Loss: 0.0063280202593887225\n",
      "Epoch: 356, Loss: 0.005823387415148318\n",
      "Epoch: 357, Loss: 0.005166787814232521\n",
      "Epoch: 358, Loss: 0.00554812366317492\n",
      "Epoch: 359, Loss: 0.0051160097937099636\n",
      "Epoch: 360, Loss: 0.00558211469615344\n",
      "Epoch: 361, Loss: 0.004365011576737743\n",
      "Epoch: 362, Loss: 0.006148502303403802\n",
      "Epoch: 363, Loss: 0.005507686495548114\n",
      "Epoch: 364, Loss: 0.006141588281025179\n",
      "Epoch: 365, Loss: 0.00546102412045002\n",
      "Epoch: 366, Loss: 0.006171050306875259\n",
      "Epoch: 367, Loss: 0.004805429482075851\n",
      "Epoch: 368, Loss: 0.005239374018856324\n",
      "Epoch: 369, Loss: 0.006119642886915244\n",
      "Epoch: 370, Loss: 0.00481948652304709\n",
      "Epoch: 371, Loss: 0.004740058095194399\n",
      "Epoch: 372, Loss: 0.005063086122390814\n",
      "Epoch: 373, Loss: 0.005199138118769042\n",
      "Epoch: 374, Loss: 0.0049401121359551325\n",
      "Epoch: 375, Loss: 0.0051509437907952815\n",
      "Epoch: 376, Loss: 0.0053659311815863475\n",
      "Epoch: 377, Loss: 0.0046876259439159185\n",
      "Epoch: 378, Loss: 0.004812076440430246\n",
      "Epoch: 379, Loss: 0.004844721974222921\n",
      "Epoch: 380, Loss: 0.005094910171465017\n",
      "Epoch: 381, Loss: 0.004096664721146226\n",
      "Epoch: 382, Loss: 0.005328975195880048\n",
      "Epoch: 383, Loss: 0.005225169617915526\n",
      "Epoch: 384, Loss: 0.005100822731037624\n",
      "Epoch: 385, Loss: 0.004459334311832208\n",
      "Epoch: 386, Loss: 0.004798919690074399\n",
      "Epoch: 387, Loss: 0.004931877498165704\n",
      "Epoch: 388, Loss: 0.004571986966766417\n",
      "Epoch: 389, Loss: 0.004889544128673151\n",
      "Epoch: 390, Loss: 0.005166327697224915\n",
      "Epoch: 391, Loss: 0.004868180243647657\n",
      "Epoch: 392, Loss: 0.004190909487078898\n",
      "Epoch: 393, Loss: 0.004391631737235002\n",
      "Epoch: 394, Loss: 0.005221487168455496\n",
      "Epoch: 395, Loss: 0.004519450129009783\n",
      "Epoch: 396, Loss: 0.004346241752500646\n",
      "Epoch: 397, Loss: 0.004039510524307843\n",
      "Epoch: 398, Loss: 0.004097626733710058\n",
      "Epoch: 399, Loss: 0.0042547149932943285\n",
      "Epoch: 400, Loss: 0.0046381077117985114\n",
      "Epoch: 401, Loss: 0.004851203222642653\n",
      "Epoch: 402, Loss: 0.004508193662331905\n",
      "Epoch: 403, Loss: 0.00439904558879789\n",
      "Epoch: 404, Loss: 0.004537539440207183\n",
      "Epoch: 405, Loss: 0.004529542740783654\n",
      "Epoch: 406, Loss: 0.004348486087110359\n",
      "Epoch: 407, Loss: 0.00537533349415753\n",
      "Epoch: 408, Loss: 0.004246135722496547\n",
      "Epoch: 409, Loss: 0.004622682121407706\n",
      "Epoch: 410, Loss: 0.004032650671433657\n",
      "Epoch: 411, Loss: 0.004047892063681502\n",
      "Epoch: 412, Loss: 0.0037772436771774665\n",
      "Epoch: 413, Loss: 0.004344709086581133\n",
      "Epoch: 414, Loss: 0.0045409802260110155\n",
      "Epoch: 415, Loss: 0.004232931722071953\n",
      "Epoch: 416, Loss: 0.004427692241733894\n",
      "Epoch: 417, Loss: 0.0036761079245479777\n",
      "Epoch: 418, Loss: 0.004409583030792419\n",
      "Epoch: 419, Loss: 0.0034389493957860395\n",
      "Epoch: 420, Loss: 0.004451906672329642\n",
      "Epoch: 421, Loss: 0.004096642027434427\n",
      "Epoch: 422, Loss: 0.004340043611591682\n",
      "Epoch: 423, Loss: 0.0033150549134006724\n",
      "Epoch: 424, Loss: 0.003996392246335745\n",
      "Epoch: 425, Loss: 0.004451586239156313\n",
      "Epoch: 426, Loss: 0.003352093161083758\n",
      "Epoch: 427, Loss: 0.00458115350920707\n",
      "Epoch: 428, Loss: 0.0038432974833995104\n",
      "Epoch: 429, Loss: 0.003678268192743417\n",
      "Epoch: 430, Loss: 0.0036926369793945923\n",
      "Epoch: 431, Loss: 0.003924374585039914\n",
      "Epoch: 432, Loss: 0.0032991107436828315\n",
      "Epoch: 433, Loss: 0.003960665060731117\n",
      "Epoch: 434, Loss: 0.004126686813833658\n",
      "Epoch: 435, Loss: 0.004647529516660143\n",
      "Epoch: 436, Loss: 0.004249014942615759\n",
      "Epoch: 437, Loss: 0.0033496164178359322\n",
      "Epoch: 438, Loss: 0.004057164682308212\n",
      "Epoch: 439, Loss: 0.004405366533319466\n",
      "Epoch: 440, Loss: 0.003420336128328927\n",
      "Epoch: 441, Loss: 0.004244786832714453\n",
      "Epoch: 442, Loss: 0.004299382359022275\n",
      "Epoch: 443, Loss: 0.003194544420694001\n",
      "Epoch: 444, Loss: 0.0037794708769069985\n",
      "Epoch: 445, Loss: 0.003715489168826025\n",
      "Epoch: 446, Loss: 0.002946151973446831\n",
      "Epoch: 447, Loss: 0.003405148243473377\n",
      "Epoch: 448, Loss: 0.0032306528664776124\n",
      "Epoch: 449, Loss: 0.003902628071955405\n",
      "Epoch: 450, Loss: 0.004085292974195909\n",
      "Epoch: 451, Loss: 0.003771494288230315\n",
      "Epoch: 452, Loss: 0.0034432232860126533\n",
      "Epoch: 453, Loss: 0.0032673297755536623\n",
      "Epoch: 454, Loss: 0.003948001263779588\n",
      "Epoch: 455, Loss: 0.0031261299081961624\n",
      "Epoch: 456, Loss: 0.003781977589824237\n",
      "Epoch: 457, Loss: 0.0041404010116821155\n",
      "Epoch: 458, Loss: 0.003299564887129236\n",
      "Epoch: 459, Loss: 0.0030841411789879203\n",
      "Epoch: 460, Loss: 0.0029546173973358236\n",
      "Epoch: 461, Loss: 0.003646861732704565\n",
      "Epoch: 462, Loss: 0.0028354698661132716\n",
      "Epoch: 463, Loss: 0.004115887451916933\n",
      "Epoch: 464, Loss: 0.003657109547930304\n",
      "Epoch: 465, Loss: 0.003115653402346652\n",
      "Epoch: 466, Loss: 0.003489090609946288\n",
      "Epoch: 467, Loss: 0.0033229622058570385\n",
      "Epoch: 468, Loss: 0.003597750255721621\n",
      "Epoch: 469, Loss: 0.0031311069760704413\n",
      "Epoch: 470, Loss: 0.0035824624937959015\n",
      "Epoch: 471, Loss: 0.003213022238924168\n",
      "Epoch: 472, Loss: 0.0032757027220213786\n",
      "Epoch: 473, Loss: 0.003149502197629772\n",
      "Epoch: 474, Loss: 0.0031092083227122203\n",
      "Epoch: 475, Loss: 0.0033154455304611474\n",
      "Epoch: 476, Loss: 0.0033091725199483335\n",
      "Epoch: 477, Loss: 0.002793052110064309\n",
      "Epoch: 478, Loss: 0.003467987153271679\n",
      "Epoch: 479, Loss: 0.0032722184259910136\n",
      "Epoch: 480, Loss: 0.0030020217309356667\n",
      "Epoch: 481, Loss: 0.00320502593240235\n",
      "Epoch: 482, Loss: 0.0027943855966441333\n",
      "Epoch: 483, Loss: 0.0035390112025197595\n",
      "Epoch: 484, Loss: 0.0027087854468845762\n",
      "Epoch: 485, Loss: 0.0038600192783633247\n",
      "Epoch: 486, Loss: 0.003001994489750359\n",
      "Epoch: 487, Loss: 0.0030316366246552207\n",
      "Epoch: 488, Loss: 0.0033109769137809053\n",
      "Epoch: 489, Loss: 0.0027082983651780523\n",
      "Epoch: 490, Loss: 0.0032183501170948148\n",
      "Epoch: 491, Loss: 0.0036451538908295333\n",
      "Epoch: 492, Loss: 0.0029523194825742394\n",
      "Epoch: 493, Loss: 0.003531736379954964\n",
      "Epoch: 494, Loss: 0.0034775496533256955\n",
      "Epoch: 495, Loss: 0.0030443493596976623\n",
      "Epoch: 496, Loss: 0.0032386636885348707\n",
      "Epoch: 497, Loss: 0.0030969012514106\n",
      "Epoch: 498, Loss: 0.002900485815189313\n",
      "Epoch: 499, Loss: 0.0025494818692095578\n",
      "Epoch: 500, Loss: 0.002956037234980613\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 500\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        outputs = model(input_ids=batch_input_ids, labels=batch_input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f'Epoch: {epoch + 1}, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_pretrained('t5-small-emoji')\n",
    "\n",
    "# Save the encoder\n",
    "torch.save(model.encoder.state_dict(), 'encoder_model.pth')\n",
    "\n",
    "# Save the decoder\n",
    "torch.save(model.decoder.state_dict(), 'decoder_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD MODEL\n",
    "# Load the encoder\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "model.encoder.load_state_dict(torch.load('encoder_model.pth'))\n",
    "model.decoder.load_state_dict(torch.load('decoder_model.pth'))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunday afternoon walking through Venice in the sun with user Abbot Kinney, Venice\n",
      "---------------------\n",
      "Sonntag Nachmittag Spa Spa Spa Spa Spa Spa Spa Spa Spa Spa Spa Spa Spa Spa\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Choose a text from your dataset\n",
    "text = texts[0]\n",
    "\n",
    "# Tokenize the text and obtain output from model\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "output = model.generate(input_ids, max_length = input_ids.shape[1])\n",
    "\n",
    "# Decode the generated text\n",
    "decoded_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(text)\n",
    "print(\"---------------------\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ene\\Desktop\\CS505\\T5_reconstruct.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ene/Desktop/CS505/T5_reconstruct.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m texts[\u001b[39m2\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ene/Desktop/CS505/T5_reconstruct.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoder_inputs_ids \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ene/Desktop/CS505/T5_reconstruct.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m decoder_inputs_ids \u001b[39m=\u001b[39m encoder_inputs_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "text = texts[2]\n",
    "encoder_inputs_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "decoder_inputs_ids = encoder_inputs_ids\n",
    "\n",
    "# Get the encoder's last hidden state (latent vector)\n",
    "encoder_outputs = model.encoder(input_ids=encoder_inputs_ids)\n",
    "latent_vector = encoder_outputs.last_hidden_state\n",
    "\n",
    "# play around\n",
    "random_tensor = torch.randn(1, 10, 512).to(device)\n",
    "encoder_outputs.last_hidden_state = random_tensor\n",
    "\n",
    "# decoder_outputs = model.decoder(input_ids=decoder_inputs_ids, encoder_hidden_states=latent_vector)\n",
    "\n",
    "outputs = model(decoder_input_ids = encoder_inputs_ids, encoder_outputs=encoder_outputs)\n",
    "logits = outputs.logits\n",
    "predicted_token_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# And use the tokenizer to convert these token IDs back into text\n",
    "predicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
